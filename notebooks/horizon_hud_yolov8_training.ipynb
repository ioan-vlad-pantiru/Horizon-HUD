{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceType":"datasetVersion","sourceId":14831116,"datasetId":9485432,"databundleVersionId":15689039}],"dockerImageVersionId":31287,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Horizon-HUD: YOLOv8n Safety Detection Training\n\n**Setup:**\n1. Upload BDD100K images as a Kaggle dataset (e.g. `bdd100k-images`) with `train/` and `val/` folders\n2. Upload BDD100K labels as a Kaggle dataset (e.g. `bdd100k-labels`) with `train/` and `val/` folders\n3. Add both as inputs to this notebook (right sidebar > Add Data)\n4. Set Accelerator to **GPU P100** or **T4 x2** in Settings\n5. Enable **Internet** in Settings\n6. Update the paths in Cell 2 below\n\n**Resume after timeout:** See Cell 2","metadata":{}},{"cell_type":"code","source":"!pip install -q ultralytics","metadata":{"execution":{"iopub.status.busy":"2026-02-19T15:21:20.832534Z","iopub.execute_input":"2026-02-19T15:21:20.832881Z","iopub.status.idle":"2026-02-19T15:21:24.891567Z","shell.execute_reply.started":"2026-02-19T15:21:20.832853Z","shell.execute_reply":"2026-02-19T15:21:24.890460Z"},"trusted":true},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# ============================================================\n# CONFIGURATION\n# ============================================================\nimport os\nfrom pathlib import Path\n\n# Show available datasets (top 3 levels only, no deep scan)\nprint(\"Available inputs:\")\nbase = Path(\"/kaggle/input\")\nfor d1 in sorted(base.iterdir()):\n    print(f\"  {d1}/\")\n    if d1.is_dir():\n        for d2 in sorted(d1.iterdir()):\n            print(f\"    {d2.name}/\")\n            if d2.is_dir():\n                for d3 in sorted(d2.iterdir()):\n                    if d3.is_dir():\n                        print(f\"      {d3.name}/  -> {sorted(os.listdir(d3))[:5]}\")\nprint()\n\n# ---- SET THESE TO MATCH THE OUTPUT ABOVE ----\nIMAGES_ROOT = \"/kaggle/input/datasets/ioanvladpantiru/100k-labels/100k/100k\"\nLABELS_ROOT = \"/kaggle/input/datasets/ioanvladpantiru/100k-labels/100k_labels/100k_labels\"\n\n# Resume: to continue training after a timeout:\n# 1. Download last.pt from previous run output\n# 2. Upload it as a new Kaggle dataset (e.g. \"horizon-checkpoint\")\n# 3. Add it as input and set the path here:\nRESUME_FROM = None  # e.g. \"/kaggle/input/horizon-checkpoint/last.pt\"\n\n# ============================================================\n# ADVANCED (usually no need to change)\n# ============================================================\nEPOCHS = 100\nIMGSZ = 640\nBATCH = -1       # -1 = auto-detect best batch size for your GPU\nWORKERS = 2","metadata":{"execution":{"iopub.status.busy":"2026-02-19T15:21:24.893917Z","iopub.execute_input":"2026-02-19T15:21:24.894334Z","iopub.status.idle":"2026-02-19T15:21:24.905335Z","shell.execute_reply.started":"2026-02-19T15:21:24.894300Z","shell.execute_reply":"2026-02-19T15:21:24.904485Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Available inputs:\n  /kaggle/input/datasets/\n    ioanvladpantiru/\n      100k-labels/  -> ['100k', '100k_labels']\n\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# ============================================================\n# PREFLIGHT CHECKS\n# ============================================================\nimport os, shutil, json, torch\nfrom pathlib import Path\n\nWORK = Path(\"/kaggle/working\")\nDATASET = WORK / \"dataset\"\nMODELS = WORK / \"models\"\nDOWNLOAD = WORK / \"download\"\n\nerrors = []\n\n# GPU\nif torch.cuda.is_available():\n    gpu = torch.cuda.get_device_name(0)\n    vram = torch.cuda.get_device_properties(0).total_memory / 1e9\n    print(f\"GPU: {gpu} ({vram:.1f} GB)\")\nelse:\n    errors.append(\"No GPU detected. Go to Settings > Accelerator and select GPU.\")\n\n# Disk\ndisk = shutil.disk_usage(str(WORK))\nfree_gb = disk.free / 1e9\nprint(f\"Disk free: {free_gb:.1f} GB\")\nif free_gb < 5:\n    errors.append(f\"Low disk space: {free_gb:.1f} GB free, need at least 5 GB.\")\n\n# Inputs\nif not RESUME_FROM:\n    for name, root in [(\"Images\", IMAGES_ROOT), (\"Labels\", LABELS_ROOT)]:\n        r = Path(root)\n        if not r.exists():\n            errors.append(f\"{name} root not found: {r}  -- Check the dataset name in Add Data.\")\n            continue\n        for split in [\"train\", \"val\"]:\n            d = r / split\n            if not d.exists():\n                errors.append(f\"{name} missing {split}/ subfolder in {r}\")\nelse:\n    p = Path(RESUME_FROM)\n    if not p.exists():\n        errors.append(f\"Resume checkpoint not found: {p}\")\n\nif errors:\n    print(\"\\n=== ERRORS ===\")\n    for e in errors:\n        print(f\"  - {e}\")\n    raise RuntimeError(\"Fix the errors above before continuing.\")\n\nif not RESUME_FROM:\n    imgs_root = Path(IMAGES_ROOT)\n    lbls_root = Path(LABELS_ROOT)\n    n_train_img = len(list((imgs_root / \"train\").glob(\"*.jpg\")))\n    n_val_img = len(list((imgs_root / \"val\").glob(\"*.jpg\")))\n    n_train_lbl = len(list((lbls_root / \"train\").glob(\"*.json\")))\n    n_val_lbl = len(list((lbls_root / \"val\").glob(\"*.json\")))\n    print(f\"Train: {n_train_img} images, {n_train_lbl} label files\")\n    print(f\"Val:   {n_val_img} images, {n_val_lbl} label files\")\n    if n_train_img == 0:\n        raise RuntimeError(\"No training images found. Check that your images dataset has train/*.jpg\")\nelse:\n    print(f\"Resuming from: {RESUME_FROM}\")\n\nprint(\"\\nAll checks passed.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T15:21:24.906522Z","iopub.execute_input":"2026-02-19T15:21:24.906794Z","iopub.status.idle":"2026-02-19T15:21:27.588494Z","shell.execute_reply.started":"2026-02-19T15:21:24.906761Z","shell.execute_reply":"2026-02-19T15:21:27.587614Z"}},"outputs":[{"name":"stdout","text":"GPU: Tesla P100-PCIE-16GB (17.1 GB)\nDisk free: 20.9 GB\nTrain: 70000 images, 70000 label files\nVal:   10000 images, 10000 label files\n\nAll checks passed.\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# ============================================================\n# PREPARE YOLO DATASET (skipped on resume)\n# ============================================================\nfrom concurrent.futures import ProcessPoolExecutor, as_completed\n\nBDD_TO_HORIZON = {\n    \"car\": 0, \"bus\": 0, \"truck\": 0, \"train\": 0,\n    \"person\": 1,\n    \"rider\": 2, \"bike\": 2, \"motor\": 2, \"motorcycle\": 2, \"bicycle\": 2,\n    \"traffic sign\": 3, \"traffic light\": 3,\n}\nIMG_W, IMG_H = 1280, 720\n\ndef convert_one(args):\n    label_path, out_dir = args\n    try:\n        with open(label_path) as f:\n            ann = json.load(f)\n    except Exception:\n        return -1\n    frames = ann.get(\"frames\", [])\n    if not frames:\n        Path(out_dir / (label_path.stem + \".txt\")).write_text(\"\")\n        return 0\n    lines = []\n    for obj in frames[0].get(\"objects\", []):\n        cls_id = BDD_TO_HORIZON.get(obj.get(\"category\", \"\").lower())\n        if cls_id is None:\n            continue\n        box = obj.get(\"box2d\", {})\n        if not box:\n            continue\n        x1 = max(0.0, float(box.get(\"x1\", 0)))\n        y1 = max(0.0, float(box.get(\"y1\", 0)))\n        x2 = min(float(IMG_W), float(box.get(\"x2\", 0)))\n        y2 = min(float(IMG_H), float(box.get(\"y2\", 0)))\n        if x2 <= x1 or y2 <= y1:\n            continue\n        cx = ((x1 + x2) / 2.0) / IMG_W\n        cy = ((y1 + y2) / 2.0) / IMG_H\n        w = (x2 - x1) / IMG_W\n        h = (y2 - y1) / IMG_H\n        lines.append(f\"{cls_id} {cx:.6f} {cy:.6f} {w:.6f} {h:.6f}\")\n    Path(out_dir / (label_path.stem + \".txt\")).write_text(\n        \"\\n\".join(lines) + (\"\\n\" if lines else \"\")\n    )\n    return len(lines)\n\nif RESUME_FROM:\n    print(\"Skipping dataset prep (resuming).\")\nelse:\n    imgs_root = Path(IMAGES_ROOT)\n    lbls_root = Path(LABELS_ROOT)\n    total_objects = 0\n    total_errors = 0\n\n    for split in [\"train\", \"val\"]:\n        img_dst = DATASET / \"images\" / split\n        lbl_dst = DATASET / \"labels\" / split\n        img_dst.mkdir(parents=True, exist_ok=True)\n        lbl_dst.mkdir(parents=True, exist_ok=True)\n\n        # Symlink each image file (keeps /images/ in the path for YOLO)\n        src_dir = imgs_root / split\n        existing = set(os.listdir(img_dst))\n        jpgs = sorted(src_dir.glob(\"*.jpg\"))\n        linked = 0\n        for jpg in jpgs:\n            if jpg.name not in existing:\n                os.symlink(str(jpg), str(img_dst / jpg.name))\n                linked += 1\n        print(f\"{split}: linked {linked} images (skipped {len(existing)} existing)\")\n\n        # Convert labels\n        json_files = sorted((lbls_root / split).glob(\"*.json\"))\n        print(f\"{split}: converting {len(json_files)} labels...\")\n        work = [(p, lbl_dst) for p in json_files]\n        with ProcessPoolExecutor(max_workers=4) as pool:\n            futures = {pool.submit(convert_one, w): w for w in work}\n            for fut in as_completed(futures):\n                n = fut.result()\n                if n < 0:\n                    total_errors += 1\n                else:\n                    total_objects += n\n\n    print(f\"\\nConverted: {total_objects} objects\")\n    if total_errors > 0:\n        print(f\"Warning: {total_errors} label files failed to parse (skipped)\")\n\n    # Verify\n    n_img = len(list((DATASET / \"images/train\").glob(\"*.jpg\")))\n    n_lbl = len(list((DATASET / \"labels/train\").glob(\"*.txt\")))\n    print(f\"Verification: {n_img} train images, {n_lbl} train labels\")\n    if n_lbl == 0:\n        raise RuntimeError(\"No labels were created. Check your labels dataset structure.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T15:21:27.589738Z","iopub.execute_input":"2026-02-19T15:21:27.590023Z","iopub.status.idle":"2026-02-19T15:23:41.119880Z","shell.execute_reply.started":"2026-02-19T15:21:27.589998Z","shell.execute_reply":"2026-02-19T15:23:41.119016Z"}},"outputs":[{"name":"stdout","text":"train: linked 70000 images (skipped 0 existing)\ntrain: converting 70000 labels...\nval: linked 10000 images (skipped 0 existing)\nval: converting 10000 labels...\n\nConverted: 1473536 objects\nVerification: 70000 train images, 70000 train labels\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# ============================================================\n# DATASET YAML (always recreated so resume works even if /working was cleared)\n# ============================================================\ndataset_yaml = DATASET / \"dataset.yaml\"\n\nif not RESUME_FROM:\n    dataset_yaml.parent.mkdir(parents=True, exist_ok=True)\n    dataset_yaml.write_text(f\"\"\"path: {DATASET}\ntrain: images/train\nval: images/val\n\nnc: 4\nnames:\n  0: vehicle\n  1: pedestrian\n  2: cyclist\n  3: road_obstacle\n\"\"\")\n    print(f\"Created: {dataset_yaml}\")\nelse:\n    print(\"Dataset yaml not needed for resume (config is inside last.pt)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T15:23:41.122048Z","iopub.execute_input":"2026-02-19T15:23:41.122637Z","iopub.status.idle":"2026-02-19T15:23:41.129166Z","shell.execute_reply.started":"2026-02-19T15:23:41.122605Z","shell.execute_reply":"2026-02-19T15:23:41.128033Z"}},"outputs":[{"name":"stdout","text":"Created: /kaggle/working/dataset/dataset.yaml\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# ============================================================\n# TRAIN\n# ============================================================\nfrom ultralytics import YOLO\n\ntrain_ok = False\ntry:\n    if RESUME_FROM:\n        print(f\"Resuming from {RESUME_FROM}\")\n        model = YOLO(RESUME_FROM)\n        model.train(resume=True)\n    else:\n        model = YOLO(\"yolov8n.pt\")\n        model.train(\n            data=str(dataset_yaml),\n            epochs=EPOCHS,\n            imgsz=IMGSZ,\n            batch=BATCH,\n            device=0,\n            project=str(MODELS),\n            name=\"horizon_v1\",\n            exist_ok=True,\n\n            # Safety-critical: maximize recall\n            conf=0.001,\n            iou=0.6,\n\n            # Augmentation for road scenes\n            hsv_h=0.015,\n            hsv_s=0.7,\n            hsv_v=0.4,\n            degrees=0.0,\n            translate=0.1,\n            scale=0.5,\n            fliplr=0.5,\n            flipud=0.0,\n            mosaic=1.0,\n            mixup=0.1,\n\n            # Training\n            optimizer=\"AdamW\",\n            lr0=0.001,\n            lrf=0.01,\n            warmup_epochs=3,\n            weight_decay=0.0005,\n            patience=20,\n            save=True,\n            save_period=5,\n            val=True,\n            plots=True,\n            verbose=True,\n            workers=WORKERS,\n        )\n    train_ok = True\n    print(\"\\nTraining completed successfully.\")\nexcept Exception as e:\n    print(f\"\\nTraining stopped: {e}\")\n    print(\"Attempting to save whatever checkpoint exists...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T15:23:41.130561Z","iopub.execute_input":"2026-02-19T15:23:41.130967Z"}},"outputs":[{"name":"stdout","text":"Creating new Ultralytics Settings v0.0.6 file ‚úÖ \nView Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\nUpdate Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.4.0/yolov8n.pt to 'yolov8n.pt': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6.2MB 69.2MB/s 0.1s\nUltralytics 8.4.14 üöÄ Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, angle=1.0, augment=False, auto_augment=randaugment, batch=-1, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=0.001, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/kaggle/working/dataset/dataset.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, end2end=None, epochs=100, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.6, keras=False, kobj=1.0, line_width=None, lr0=0.001, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.1, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=0.0, name=horizon_v1, nbs=64, nms=False, opset=None, optimize=False, optimizer=AdamW, overlap_mask=True, patience=20, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=/kaggle/working/models, rect=False, resume=False, retina_masks=False, rle=1.0, save=True, save_conf=False, save_crop=False, save_dir=/kaggle/working/models/horizon_v1, save_frames=False, save_json=False, save_period=5, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3, warmup_momentum=0.8, weight_decay=0.0005, workers=2, workspace=None\n\u001b[KDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 755.1KB 30.2MB/s 0.0s\nOverriding model.yaml nc=80 with nc=4\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 22        [15, 18, 21]  1    752092  ultralytics.nn.modules.head.Detect           [4, 16, None, [64, 128, 256]] \nModel summary: 130 layers, 3,011,628 parameters, 3,011,612 gradients, 8.2 GFLOPs\n\nTransferred 319/355 items from pretrained weights\nFreezing layer 'model.22.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26n.pt to 'yolo26n.pt': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5.3MB 107.2MB/s 0.0s\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.6¬±0.1 ms, read: 9.7¬±3.4 MB/s, size: 55.7 KB)\n\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/working/dataset/labels/train... 70000 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 70000/70000 490.2it/s 2:23<0.0s\n\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/dataset/images/train/75055858-7d04a650.jpg: 1 duplicate labels removed\n\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /kaggle/working/dataset/labels/train.cache\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n\u001b[34m\u001b[1mAutoBatch: \u001b[0mComputing optimal batch size for imgsz=640 at 60.0% CUDA memory utilization.\n\u001b[34m\u001b[1mAutoBatch: \u001b[0mCUDA:0 (Tesla P100-PCIE-16GB) 15.89G total, 0.10G reserved, 0.06G allocated, 15.73G free\n      Params      GFLOPs  GPU_mem (GB)  forward (ms) backward (ms)                   input                  output\n     3011628       8.197         0.419          48.7           nan        (1, 3, 640, 640)                    list\n     3011628       16.39         0.818          27.1           nan        (2, 3, 640, 640)                    list\n     3011628       32.79         1.367         25.04           nan        (4, 3, 640, 640)                    list\n     3011628       65.58         2.600         30.27           nan        (8, 3, 640, 640)                    list\n     3011628       131.2         5.035         44.81           nan       (16, 3, 640, 640)                    list\n\u001b[34m\u001b[1mAutoBatch: \u001b[0mUsing batch-size 28 for CUDA:0 8.86G/15.89G (56%) ‚úÖ\n\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 100.7¬±23.8 MB/s, size: 60.5 KB)\n\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/working/dataset/labels/train.cache... 70000 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 70000/70000 14.0Git/s 0.0s\n\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/dataset/images/train/75055858-7d04a650.jpg: 1 duplicate labels removed\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.5¬±0.3 ms, read: 8.7¬±3.6 MB/s, size: 64.9 KB)\n\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/working/dataset/labels/val... 10000 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 10000/10000 469.7it/s 21.3s.1s\n\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /kaggle/working/dataset/labels/val.cache\n\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001, momentum=0.937) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0004375), 63 bias(decay=0.0)\nPlotting labels to /kaggle/working/models/horizon_v1/labels.jpg... \nImage sizes 640 train, 640 val\nUsing 2 dataloader workers\nLogging results to \u001b[1m/kaggle/working/models/horizon_v1\u001b[0m\nStarting training for 100 epochs...\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K      1/100      5.71G      1.781      1.538      1.117       1105        640: 18% ‚îÅ‚îÅ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 446/2500 2.6it/s 2:58<13:19","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# SAVE OUTPUTS (runs even if training was interrupted)\n# ============================================================\nDOWNLOAD.mkdir(exist_ok=True)\n\nweights_dir = MODELS / \"horizon_v1\" / \"weights\"\nif not weights_dir.exists():\n    # Check default ultralytics save location\n    alt = Path(\"/kaggle/working/runs/detect/horizon_v1/weights\")\n    if alt.exists():\n        weights_dir = alt\n\nsaved = []\nif weights_dir.exists():\n    for name in [\"best.pt\", \"last.pt\"]:\n        src = weights_dir / name\n        if src.exists():\n            shutil.copy(src, DOWNLOAD / name)\n            size_mb = src.stat().st_size / 1e6\n            saved.append(f\"{name} ({size_mb:.1f} MB)\")\nelse:\n    print(\"No weights directory found.\")\n\n# Training plots\nresults_dir = weights_dir.parent if weights_dir.exists() else None\nif results_dir and results_dir.exists():\n    for f in results_dir.glob(\"*.png\"):\n        shutil.copy(f, DOWNLOAD / f.name)\n        saved.append(f.name)\n    for f in results_dir.glob(\"*.csv\"):\n        shutil.copy(f, DOWNLOAD / f.name)\n        saved.append(f.name)\n\nprint(f\"Saved to {DOWNLOAD}:\")\nfor s in saved:\n    print(f\"  {s}\")\n\nif not saved:\n    print(\"Nothing saved - training may not have produced any checkpoints.\")\nelif \"last.pt\" in [s.split()[0] for s in saved]:\n    print(\"\\nTo resume: upload last.pt as a Kaggle dataset and set RESUME_FROM in Cell 2.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# EXPORT TO TFLITE (only if training completed)\n# ============================================================\nbest_path = DOWNLOAD / \"best.pt\"\nif best_path.exists():\n    try:\n        best = YOLO(str(best_path))\n        best.export(format=\"tflite\", imgsz=320, int8=True)\n        # Find the exported file and copy to download\n        for f in Path(\".\").rglob(\"*_saved_model/*.tflite\"):\n            shutil.copy(f, DOWNLOAD / \"horizon_v1_int8.tflite\")\n            print(f\"TFLite exported: {DOWNLOAD / 'horizon_v1_int8.tflite'}\")\n            break\n        else:\n            for f in Path(\".\").rglob(\"*.tflite\"):\n                shutil.copy(f, DOWNLOAD / f.name)\n                print(f\"TFLite exported: {DOWNLOAD / f.name}\")\n                break\n    except Exception as e:\n        print(f\"TFLite export failed: {e}\")\n        print(\"You can export locally: yolo export model=best.pt format=tflite imgsz=320 int8\")\nelse:\n    print(\"No best.pt found, skipping export.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# QUICK VISUAL TEST (only if best.pt exists)\n# ============================================================\nif best_path.exists():\n    import glob\n    val_dir = DATASET / \"images\" / \"val\"\n    if not val_dir.exists():\n        val_dir = Path(IMAGES_ROOT) / \"val\"\n    test_imgs = sorted(glob.glob(str(val_dir / \"*.jpg\")))[:5]\n    if test_imgs:\n        best = YOLO(str(best_path))\n        results = best.predict(\n            source=test_imgs, conf=0.25, iou=0.6, imgsz=640,\n            save=True, project=str(DOWNLOAD), name=\"test_predictions\", exist_ok=True\n        )\n        for r in results:\n            n = len(r.boxes) if r.boxes is not None else 0\n            print(f\"{Path(r.path).name}: {n} detections\")\n    else:\n        print(\"No val images found for testing.\")\nelse:\n    print(\"No best.pt, skipping test.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# FINAL SUMMARY\n# ============================================================\nprint(\"=\" * 50)\nprint(\"DOWNLOAD FILES:\")\nif DOWNLOAD.exists():\n    for f in sorted(DOWNLOAD.rglob(\"*\")):\n        if f.is_file():\n            size = f.stat().st_size / 1e6\n            print(f\"  {f.relative_to(DOWNLOAD)}  ({size:.1f} MB)\")\nprint(\"=\" * 50)\nif (DOWNLOAD / \"last.pt\").exists() and not train_ok:\n    print(\"\\nTraining was interrupted!\")\n    print(\"To resume:\")\n    print(\"  1. Download last.pt from this notebook's output\")\n    print(\"  2. Create a new Kaggle dataset from it\")\n    print(\"  3. Set RESUME_FROM in Cell 2 to the path\")\n    print(\"  4. Run all cells again\")\nelif train_ok:\n    print(\"\\nTraining completed! Download best.pt and the .tflite for your Pi5.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}