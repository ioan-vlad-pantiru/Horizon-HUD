{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Horizon-HUD: YOLOv8n Safety Detection Training\n",
        "\n",
        "**Setup:**\n",
        "1. Upload BDD100K images as a Kaggle dataset (e.g. `bdd100k-images`) with `train/` and `val/` folders\n",
        "2. Upload BDD100K labels as a Kaggle dataset (e.g. `bdd100k-labels`) with `train/` and `val/` folders\n",
        "3. Add both as inputs to this notebook (right sidebar > Add Data)\n",
        "4. Set Accelerator to **GPU P100** or **T4 x2** in Settings\n",
        "5. Enable **Internet** in Settings\n",
        "6. Update the paths in Cell 2 below\n",
        "\n",
        "**Resume after timeout:** See Cell 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-19T14:55:44.297908Z",
          "iopub.status.busy": "2026-02-19T14:55:44.297299Z",
          "iopub.status.idle": "2026-02-19T14:55:48.156854Z",
          "shell.execute_reply": "2026-02-19T14:55:48.155729Z",
          "shell.execute_reply.started": "2026-02-19T14:55:44.297874Z"
        },
        "trusted": true
      },
      "source": [
        "!pip install -q ultralytics"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-19T14:55:48.159221Z",
          "iopub.status.busy": "2026-02-19T14:55:48.158895Z"
        },
        "trusted": true
      },
      "source": [
        "# ============================================================\n",
        "# CONFIGURATION\n",
        "# ============================================================\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Show available datasets (top 3 levels only, no deep scan)\n",
        "print(\"Available inputs:\")\n",
        "base = Path(\"/kaggle/input\")\n",
        "for d1 in sorted(base.iterdir()):\n",
        "    print(f\"  {d1}/\")\n",
        "    if d1.is_dir():\n",
        "        for d2 in sorted(d1.iterdir()):\n",
        "            print(f\"    {d2.name}/\")\n",
        "            if d2.is_dir():\n",
        "                for d3 in sorted(d2.iterdir()):\n",
        "                    if d3.is_dir():\n",
        "                        print(f\"      {d3.name}/  -> {sorted(os.listdir(d3))[:5]}\")\n",
        "print()\n",
        "\n",
        "# ---- SET THESE TO MATCH THE OUTPUT ABOVE ----\n",
        "IMAGES_ROOT = \"/kaggle/input/100k-labels/100k/100k\"\n",
        "LABELS_ROOT = \"/kaggle/input/100k-labels/100k_labels/100k_labels\"\n",
        "\n",
        "# Resume: to continue training after a timeout:\n",
        "# 1. Download last.pt from previous run output\n",
        "# 2. Upload it as a new Kaggle dataset (e.g. \"horizon-checkpoint\")\n",
        "# 3. Add it as input and set the path here:\n",
        "RESUME_FROM = None  # e.g. \"/kaggle/input/horizon-checkpoint/last.pt\"\n",
        "\n",
        "# ============================================================\n",
        "# ADVANCED (usually no need to change)\n",
        "# ============================================================\n",
        "EPOCHS = 100\n",
        "IMGSZ = 640\n",
        "BATCH = -1       # -1 = auto-detect best batch size for your GPU\n",
        "WORKERS = 2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Scanning /kaggle/input/ ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true
      },
      "source": [
        "# ============================================================\n",
        "# PREFLIGHT CHECKS\n",
        "# ============================================================\n",
        "import os, shutil, json, torch\n",
        "from pathlib import Path\n",
        "\n",
        "WORK = Path(\"/kaggle/working\")\n",
        "DATASET = WORK / \"dataset\"\n",
        "MODELS = WORK / \"models\"\n",
        "DOWNLOAD = WORK / \"download\"\n",
        "\n",
        "errors = []\n",
        "\n",
        "# GPU\n",
        "if torch.cuda.is_available():\n",
        "    gpu = torch.cuda.get_device_name(0)\n",
        "    vram = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"GPU: {gpu} ({vram:.1f} GB)\")\n",
        "else:\n",
        "    errors.append(\"No GPU detected. Go to Settings > Accelerator and select GPU.\")\n",
        "\n",
        "# Disk\n",
        "disk = shutil.disk_usage(str(WORK))\n",
        "free_gb = disk.free / 1e9\n",
        "print(f\"Disk free: {free_gb:.1f} GB\")\n",
        "if free_gb < 5:\n",
        "    errors.append(f\"Low disk space: {free_gb:.1f} GB free, need at least 5 GB.\")\n",
        "\n",
        "# Inputs\n",
        "if not RESUME_FROM:\n",
        "    for name, root in [(\"Images\", IMAGES_ROOT), (\"Labels\", LABELS_ROOT)]:\n",
        "        r = Path(root)\n",
        "        if not r.exists():\n",
        "            errors.append(f\"{name} root not found: {r}  -- Check the dataset name in Add Data.\")\n",
        "            continue\n",
        "        for split in [\"train\", \"val\"]:\n",
        "            d = r / split\n",
        "            if not d.exists():\n",
        "                errors.append(f\"{name} missing {split}/ subfolder in {r}\")\n",
        "else:\n",
        "    p = Path(RESUME_FROM)\n",
        "    if not p.exists():\n",
        "        errors.append(f\"Resume checkpoint not found: {p}\")\n",
        "\n",
        "if errors:\n",
        "    print(\"\\n=== ERRORS ===\")\n",
        "    for e in errors:\n",
        "        print(f\"  - {e}\")\n",
        "    raise RuntimeError(\"Fix the errors above before continuing.\")\n",
        "\n",
        "if not RESUME_FROM:\n",
        "    imgs_root = Path(IMAGES_ROOT)\n",
        "    lbls_root = Path(LABELS_ROOT)\n",
        "    n_train_img = len(list((imgs_root / \"train\").glob(\"*.jpg\")))\n",
        "    n_val_img = len(list((imgs_root / \"val\").glob(\"*.jpg\")))\n",
        "    n_train_lbl = len(list((lbls_root / \"train\").glob(\"*.json\")))\n",
        "    n_val_lbl = len(list((lbls_root / \"val\").glob(\"*.json\")))\n",
        "    print(f\"Train: {n_train_img} images, {n_train_lbl} label files\")\n",
        "    print(f\"Val:   {n_val_img} images, {n_val_lbl} label files\")\n",
        "    if n_train_img == 0:\n",
        "        raise RuntimeError(\"No training images found. Check that your images dataset has train/*.jpg\")\n",
        "else:\n",
        "    print(f\"Resuming from: {RESUME_FROM}\")\n",
        "\n",
        "print(\"\\nAll checks passed.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true
      },
      "source": [
        "# ============================================================\n",
        "# PREPARE YOLO DATASET (skipped on resume)\n",
        "# ============================================================\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "\n",
        "BDD_TO_HORIZON = {\n",
        "    \"car\": 0, \"bus\": 0, \"truck\": 0, \"train\": 0,\n",
        "    \"person\": 1,\n",
        "    \"rider\": 2, \"bike\": 2, \"motor\": 2, \"motorcycle\": 2, \"bicycle\": 2,\n",
        "    \"traffic sign\": 3, \"traffic light\": 3,\n",
        "}\n",
        "IMG_W, IMG_H = 1280, 720\n",
        "\n",
        "def convert_one(args):\n",
        "    label_path, out_dir = args\n",
        "    try:\n",
        "        with open(label_path) as f:\n",
        "            ann = json.load(f)\n",
        "    except Exception:\n",
        "        return -1\n",
        "    frames = ann.get(\"frames\", [])\n",
        "    if not frames:\n",
        "        Path(out_dir / (label_path.stem + \".txt\")).write_text(\"\")\n",
        "        return 0\n",
        "    lines = []\n",
        "    for obj in frames[0].get(\"objects\", []):\n",
        "        cls_id = BDD_TO_HORIZON.get(obj.get(\"category\", \"\").lower())\n",
        "        if cls_id is None:\n",
        "            continue\n",
        "        box = obj.get(\"box2d\", {})\n",
        "        if not box:\n",
        "            continue\n",
        "        x1 = max(0.0, float(box.get(\"x1\", 0)))\n",
        "        y1 = max(0.0, float(box.get(\"y1\", 0)))\n",
        "        x2 = min(float(IMG_W), float(box.get(\"x2\", 0)))\n",
        "        y2 = min(float(IMG_H), float(box.get(\"y2\", 0)))\n",
        "        if x2 <= x1 or y2 <= y1:\n",
        "            continue\n",
        "        cx = ((x1 + x2) / 2.0) / IMG_W\n",
        "        cy = ((y1 + y2) / 2.0) / IMG_H\n",
        "        w = (x2 - x1) / IMG_W\n",
        "        h = (y2 - y1) / IMG_H\n",
        "        lines.append(f\"{cls_id} {cx:.6f} {cy:.6f} {w:.6f} {h:.6f}\")\n",
        "    Path(out_dir / (label_path.stem + \".txt\")).write_text(\n",
        "        \"\\n\".join(lines) + (\"\\n\" if lines else \"\")\n",
        "    )\n",
        "    return len(lines)\n",
        "\n",
        "if RESUME_FROM:\n",
        "    print(\"Skipping dataset prep (resuming).\")\n",
        "else:\n",
        "    imgs_root = Path(IMAGES_ROOT)\n",
        "    lbls_root = Path(LABELS_ROOT)\n",
        "    total_objects = 0\n",
        "    total_errors = 0\n",
        "\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        img_dst = DATASET / \"images\" / split\n",
        "        lbl_dst = DATASET / \"labels\" / split\n",
        "        img_dst.mkdir(parents=True, exist_ok=True)\n",
        "        lbl_dst.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Symlink each image file (keeps /images/ in the path for YOLO)\n",
        "        src_dir = imgs_root / split\n",
        "        existing = set(os.listdir(img_dst))\n",
        "        jpgs = sorted(src_dir.glob(\"*.jpg\"))\n",
        "        linked = 0\n",
        "        for jpg in jpgs:\n",
        "            if jpg.name not in existing:\n",
        "                os.symlink(str(jpg), str(img_dst / jpg.name))\n",
        "                linked += 1\n",
        "        print(f\"{split}: linked {linked} images (skipped {len(existing)} existing)\")\n",
        "\n",
        "        # Convert labels\n",
        "        json_files = sorted((lbls_root / split).glob(\"*.json\"))\n",
        "        print(f\"{split}: converting {len(json_files)} labels...\")\n",
        "        work = [(p, lbl_dst) for p in json_files]\n",
        "        with ProcessPoolExecutor(max_workers=4) as pool:\n",
        "            futures = {pool.submit(convert_one, w): w for w in work}\n",
        "            for fut in as_completed(futures):\n",
        "                n = fut.result()\n",
        "                if n < 0:\n",
        "                    total_errors += 1\n",
        "                else:\n",
        "                    total_objects += n\n",
        "\n",
        "    print(f\"\\nConverted: {total_objects} objects\")\n",
        "    if total_errors > 0:\n",
        "        print(f\"Warning: {total_errors} label files failed to parse (skipped)\")\n",
        "\n",
        "    # Verify\n",
        "    n_img = len(list((DATASET / \"images/train\").glob(\"*.jpg\")))\n",
        "    n_lbl = len(list((DATASET / \"labels/train\").glob(\"*.txt\")))\n",
        "    print(f\"Verification: {n_img} train images, {n_lbl} train labels\")\n",
        "    if n_lbl == 0:\n",
        "        raise RuntimeError(\"No labels were created. Check your labels dataset structure.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true
      },
      "source": [
        "# ============================================================\n",
        "# DATASET YAML (always recreated so resume works even if /working was cleared)\n",
        "# ============================================================\n",
        "dataset_yaml = DATASET / \"dataset.yaml\"\n",
        "\n",
        "if not RESUME_FROM:\n",
        "    dataset_yaml.parent.mkdir(parents=True, exist_ok=True)\n",
        "    dataset_yaml.write_text(f\"\"\"path: {DATASET}\n",
        "train: images/train\n",
        "val: images/val\n",
        "\n",
        "nc: 4\n",
        "names:\n",
        "  0: vehicle\n",
        "  1: pedestrian\n",
        "  2: cyclist\n",
        "  3: road_obstacle\n",
        "\"\"\")\n",
        "    print(f\"Created: {dataset_yaml}\")\n",
        "else:\n",
        "    print(\"Dataset yaml not needed for resume (config is inside last.pt)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true
      },
      "source": [
        "# ============================================================\n",
        "# TRAIN\n",
        "# ============================================================\n",
        "from ultralytics import YOLO\n",
        "\n",
        "train_ok = False\n",
        "try:\n",
        "    if RESUME_FROM:\n",
        "        print(f\"Resuming from {RESUME_FROM}\")\n",
        "        model = YOLO(RESUME_FROM)\n",
        "        model.train(resume=True)\n",
        "    else:\n",
        "        model = YOLO(\"yolov8n.pt\")\n",
        "        model.train(\n",
        "            data=str(dataset_yaml),\n",
        "            epochs=EPOCHS,\n",
        "            imgsz=IMGSZ,\n",
        "            batch=BATCH,\n",
        "            device=0,\n",
        "            project=str(MODELS),\n",
        "            name=\"horizon_v1\",\n",
        "            exist_ok=True,\n",
        "\n",
        "            # Safety-critical: maximize recall\n",
        "            conf=0.001,\n",
        "            iou=0.6,\n",
        "\n",
        "            # Augmentation for road scenes\n",
        "            hsv_h=0.015,\n",
        "            hsv_s=0.7,\n",
        "            hsv_v=0.4,\n",
        "            degrees=0.0,\n",
        "            translate=0.1,\n",
        "            scale=0.5,\n",
        "            fliplr=0.5,\n",
        "            flipud=0.0,\n",
        "            mosaic=1.0,\n",
        "            mixup=0.1,\n",
        "\n",
        "            # Training\n",
        "            optimizer=\"AdamW\",\n",
        "            lr0=0.001,\n",
        "            lrf=0.01,\n",
        "            warmup_epochs=3,\n",
        "            weight_decay=0.0005,\n",
        "            patience=20,\n",
        "            save=True,\n",
        "            save_period=5,\n",
        "            val=True,\n",
        "            plots=True,\n",
        "            verbose=True,\n",
        "            workers=WORKERS,\n",
        "        )\n",
        "    train_ok = True\n",
        "    print(\"\\nTraining completed successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nTraining stopped: {e}\")\n",
        "    print(\"Attempting to save whatever checkpoint exists...\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true
      },
      "source": [
        "# ============================================================\n",
        "# SAVE OUTPUTS (runs even if training was interrupted)\n",
        "# ============================================================\n",
        "DOWNLOAD.mkdir(exist_ok=True)\n",
        "\n",
        "weights_dir = MODELS / \"horizon_v1\" / \"weights\"\n",
        "if not weights_dir.exists():\n",
        "    # Check default ultralytics save location\n",
        "    alt = Path(\"/kaggle/working/runs/detect/horizon_v1/weights\")\n",
        "    if alt.exists():\n",
        "        weights_dir = alt\n",
        "\n",
        "saved = []\n",
        "if weights_dir.exists():\n",
        "    for name in [\"best.pt\", \"last.pt\"]:\n",
        "        src = weights_dir / name\n",
        "        if src.exists():\n",
        "            shutil.copy(src, DOWNLOAD / name)\n",
        "            size_mb = src.stat().st_size / 1e6\n",
        "            saved.append(f\"{name} ({size_mb:.1f} MB)\")\n",
        "else:\n",
        "    print(\"No weights directory found.\")\n",
        "\n",
        "# Training plots\n",
        "results_dir = weights_dir.parent if weights_dir.exists() else None\n",
        "if results_dir and results_dir.exists():\n",
        "    for f in results_dir.glob(\"*.png\"):\n",
        "        shutil.copy(f, DOWNLOAD / f.name)\n",
        "        saved.append(f.name)\n",
        "    for f in results_dir.glob(\"*.csv\"):\n",
        "        shutil.copy(f, DOWNLOAD / f.name)\n",
        "        saved.append(f.name)\n",
        "\n",
        "print(f\"Saved to {DOWNLOAD}:\")\n",
        "for s in saved:\n",
        "    print(f\"  {s}\")\n",
        "\n",
        "if not saved:\n",
        "    print(\"Nothing saved - training may not have produced any checkpoints.\")\n",
        "elif \"last.pt\" in [s.split()[0] for s in saved]:\n",
        "    print(\"\\nTo resume: upload last.pt as a Kaggle dataset and set RESUME_FROM in Cell 2.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true
      },
      "source": [
        "# ============================================================\n",
        "# EXPORT TO TFLITE (only if training completed)\n",
        "# ============================================================\n",
        "best_path = DOWNLOAD / \"best.pt\"\n",
        "if best_path.exists():\n",
        "    try:\n",
        "        best = YOLO(str(best_path))\n",
        "        best.export(format=\"tflite\", imgsz=320, int8=True)\n",
        "        # Find the exported file and copy to download\n",
        "        for f in Path(\".\").rglob(\"*_saved_model/*.tflite\"):\n",
        "            shutil.copy(f, DOWNLOAD / \"horizon_v1_int8.tflite\")\n",
        "            print(f\"TFLite exported: {DOWNLOAD / 'horizon_v1_int8.tflite'}\")\n",
        "            break\n",
        "        else:\n",
        "            for f in Path(\".\").rglob(\"*.tflite\"):\n",
        "                shutil.copy(f, DOWNLOAD / f.name)\n",
        "                print(f\"TFLite exported: {DOWNLOAD / f.name}\")\n",
        "                break\n",
        "    except Exception as e:\n",
        "        print(f\"TFLite export failed: {e}\")\n",
        "        print(\"You can export locally: yolo export model=best.pt format=tflite imgsz=320 int8\")\n",
        "else:\n",
        "    print(\"No best.pt found, skipping export.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true
      },
      "source": [
        "# ============================================================\n",
        "# QUICK VISUAL TEST (only if best.pt exists)\n",
        "# ============================================================\n",
        "if best_path.exists():\n",
        "    import glob\n",
        "    val_dir = DATASET / \"images\" / \"val\"\n",
        "    if not val_dir.exists():\n",
        "        val_dir = Path(IMAGES_ROOT) / \"val\"\n",
        "    test_imgs = sorted(glob.glob(str(val_dir / \"*.jpg\")))[:5]\n",
        "    if test_imgs:\n",
        "        best = YOLO(str(best_path))\n",
        "        results = best.predict(\n",
        "            source=test_imgs, conf=0.25, iou=0.6, imgsz=640,\n",
        "            save=True, project=str(DOWNLOAD), name=\"test_predictions\", exist_ok=True\n",
        "        )\n",
        "        for r in results:\n",
        "            n = len(r.boxes) if r.boxes is not None else 0\n",
        "            print(f\"{Path(r.path).name}: {n} detections\")\n",
        "    else:\n",
        "        print(\"No val images found for testing.\")\n",
        "else:\n",
        "    print(\"No best.pt, skipping test.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true
      },
      "source": [
        "# ============================================================\n",
        "# FINAL SUMMARY\n",
        "# ============================================================\n",
        "print(\"=\" * 50)\n",
        "print(\"DOWNLOAD FILES:\")\n",
        "if DOWNLOAD.exists():\n",
        "    for f in sorted(DOWNLOAD.rglob(\"*\")):\n",
        "        if f.is_file():\n",
        "            size = f.stat().st_size / 1e6\n",
        "            print(f\"  {f.relative_to(DOWNLOAD)}  ({size:.1f} MB)\")\n",
        "print(\"=\" * 50)\n",
        "if (DOWNLOAD / \"last.pt\").exists() and not train_ok:\n",
        "    print(\"\\nTraining was interrupted!\")\n",
        "    print(\"To resume:\")\n",
        "    print(\"  1. Download last.pt from this notebook's output\")\n",
        "    print(\"  2. Create a new Kaggle dataset from it\")\n",
        "    print(\"  3. Set RESUME_FROM in Cell 2 to the path\")\n",
        "    print(\"  4. Run all cells again\")\n",
        "elif train_ok:\n",
        "    print(\"\\nTraining completed! Download best.pt and the .tflite for your Pi5.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "databundleVersionId": 15689039,
          "datasetId": 9485432,
          "sourceId": 14831116,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31287,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}