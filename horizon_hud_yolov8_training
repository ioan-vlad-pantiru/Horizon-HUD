{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceType":"datasetVersion","sourceId":14831116,"datasetId":9485432,"databundleVersionId":15689039}],"dockerImageVersionId":31287,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Horizon-HUD: YOLOv8n Safety Detection Training\n\n**Setup:**\n1. Upload BDD100K images as a Kaggle dataset (e.g. `bdd100k-images`) with `train/` and `val/` folders\n2. Upload BDD100K labels as a Kaggle dataset (e.g. `bdd100k-labels`) with `train/` and `val/` folders\n3. Add both as inputs to this notebook (right sidebar > Add Data)\n4. Set Accelerator to **GPU P100** or **T4 x2** in Settings\n5. Enable **Internet** in Settings\n6. Update the paths in Cell 2 below\n\n**Resume after timeout:** See Cell 2","metadata":{}},{"cell_type":"code","source":"!pip install -q ultralytics","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# CONFIGURATION\n# ============================================================\nimport os\nfrom pathlib import Path\n\n# Auto-discover dataset paths\nIMAGES_ROOT = None\nLABELS_ROOT = None\n\nprint('Scanning /kaggle/input/ ...')\nfor root, dirs, files in os.walk('/kaggle/input'):\n    # Images: find a dir containing train/ with .jpg files\n    if not IMAGES_ROOT and 'train' in dirs:\n        train_dir = os.path.join(root, 'train')\n        if any(f.endswith('.jpg') for f in os.listdir(train_dir)[:10]):\n            IMAGES_ROOT = root\n    # Labels: find a dir containing train/ with .json files\n    if not LABELS_ROOT and 'train' in dirs:\n        train_dir = os.path.join(root, 'train')\n        if any(f.endswith('.json') for f in os.listdir(train_dir)[:10]):\n            LABELS_ROOT = root\n\nprint(f'  Images: {IMAGES_ROOT}')\nprint(f'  Labels: {LABELS_ROOT}')\n\n# Manual override if auto-detect fails:\n# IMAGES_ROOT = \"/kaggle/input/YOUR_DATASET/path/to/images\"\n# LABELS_ROOT = \"/kaggle/input/YOUR_DATASET/path/to/labels\"\n\n# Resume: to continue training after a timeout:\n# 1. Download last.pt from previous run output\n# 2. Upload it as a new Kaggle dataset (e.g. \"horizon-checkpoint\")\n# 3. Add it as input and set the path here:\nRESUME_FROM = None  # e.g. \"/kaggle/input/horizon-checkpoint/last.pt\"\n\n# ============================================================\n# ADVANCED (usually no need to change)\n# ============================================================\nEPOCHS = 100\nIMGSZ = 640\nBATCH = -1       # -1 = auto-detect best batch size for your GPU\nWORKERS = 2","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# PREFLIGHT CHECKS\n# ============================================================\nimport os, shutil, json, torch\nfrom pathlib import Path\n\nWORK = Path(\"/kaggle/working\")\nDATASET = WORK / \"dataset\"\nMODELS = WORK / \"models\"\nDOWNLOAD = WORK / \"download\"\n\nerrors = []\n\n# GPU\nif torch.cuda.is_available():\n    gpu = torch.cuda.get_device_name(0)\n    vram = torch.cuda.get_device_properties(0).total_memory / 1e9\n    print(f\"GPU: {gpu} ({vram:.1f} GB)\")\nelse:\n    errors.append(\"No GPU detected. Go to Settings > Accelerator and select GPU.\")\n\n# Disk\ndisk = shutil.disk_usage(str(WORK))\nfree_gb = disk.free / 1e9\nprint(f\"Disk free: {free_gb:.1f} GB\")\nif free_gb < 5:\n    errors.append(f\"Low disk space: {free_gb:.1f} GB free, need at least 5 GB.\")\n\n# Inputs\nif not RESUME_FROM:\n    for name, root in [(\"Images\", IMAGES_ROOT), (\"Labels\", LABELS_ROOT)]:\n        r = Path(root)\n        if not r.exists():\n            errors.append(f\"{name} root not found: {r}  -- Check the dataset name in Add Data.\")\n            continue\n        for split in [\"train\", \"val\"]:\n            d = r / split\n            if not d.exists():\n                errors.append(f\"{name} missing {split}/ subfolder in {r}\")\nelse:\n    p = Path(RESUME_FROM)\n    if not p.exists():\n        errors.append(f\"Resume checkpoint not found: {p}\")\n\nif errors:\n    print(\"\\n=== ERRORS ===\")\n    for e in errors:\n        print(f\"  - {e}\")\n    raise RuntimeError(\"Fix the errors above before continuing.\")\n\nif not RESUME_FROM:\n    imgs_root = Path(IMAGES_ROOT)\n    lbls_root = Path(LABELS_ROOT)\n    n_train_img = len(list((imgs_root / \"train\").glob(\"*.jpg\")))\n    n_val_img = len(list((imgs_root / \"val\").glob(\"*.jpg\")))\n    n_train_lbl = len(list((lbls_root / \"train\").glob(\"*.json\")))\n    n_val_lbl = len(list((lbls_root / \"val\").glob(\"*.json\")))\n    print(f\"Train: {n_train_img} images, {n_train_lbl} label files\")\n    print(f\"Val:   {n_val_img} images, {n_val_lbl} label files\")\n    if n_train_img == 0:\n        raise RuntimeError(\"No training images found. Check that your images dataset has train/*.jpg\")\nelse:\n    print(f\"Resuming from: {RESUME_FROM}\")\n\nprint(\"\\nAll checks passed.\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# PREPARE YOLO DATASET (skipped on resume)\n# ============================================================\nfrom concurrent.futures import ProcessPoolExecutor, as_completed\n\nBDD_TO_HORIZON = {\n    \"car\": 0, \"bus\": 0, \"truck\": 0, \"train\": 0,\n    \"person\": 1,\n    \"rider\": 2, \"bike\": 2, \"motor\": 2, \"motorcycle\": 2, \"bicycle\": 2,\n    \"traffic sign\": 3, \"traffic light\": 3,\n}\nIMG_W, IMG_H = 1280, 720\n\ndef convert_one(args):\n    label_path, out_dir = args\n    try:\n        with open(label_path) as f:\n            ann = json.load(f)\n    except Exception:\n        return -1\n    frames = ann.get(\"frames\", [])\n    if not frames:\n        Path(out_dir / (label_path.stem + \".txt\")).write_text(\"\")\n        return 0\n    lines = []\n    for obj in frames[0].get(\"objects\", []):\n        cls_id = BDD_TO_HORIZON.get(obj.get(\"category\", \"\").lower())\n        if cls_id is None:\n            continue\n        box = obj.get(\"box2d\", {})\n        if not box:\n            continue\n        x1 = max(0.0, float(box.get(\"x1\", 0)))\n        y1 = max(0.0, float(box.get(\"y1\", 0)))\n        x2 = min(float(IMG_W), float(box.get(\"x2\", 0)))\n        y2 = min(float(IMG_H), float(box.get(\"y2\", 0)))\n        if x2 <= x1 or y2 <= y1:\n            continue\n        cx = ((x1 + x2) / 2.0) / IMG_W\n        cy = ((y1 + y2) / 2.0) / IMG_H\n        w = (x2 - x1) / IMG_W\n        h = (y2 - y1) / IMG_H\n        lines.append(f\"{cls_id} {cx:.6f} {cy:.6f} {w:.6f} {h:.6f}\")\n    Path(out_dir / (label_path.stem + \".txt\")).write_text(\n        \"\\n\".join(lines) + (\"\\n\" if lines else \"\")\n    )\n    return len(lines)\n\nif RESUME_FROM:\n    print(\"Skipping dataset prep (resuming).\")\nelse:\n    imgs_root = Path(IMAGES_ROOT)\n    lbls_root = Path(LABELS_ROOT)\n    total_objects = 0\n    total_errors = 0\n\n    for split in [\"train\", \"val\"]:\n        img_dst = DATASET / \"images\" / split\n        lbl_dst = DATASET / \"labels\" / split\n        img_dst.mkdir(parents=True, exist_ok=True)\n        lbl_dst.mkdir(parents=True, exist_ok=True)\n\n        # Symlink each image file (keeps /images/ in the path for YOLO)\n        src_dir = imgs_root / split\n        existing = set(os.listdir(img_dst))\n        jpgs = sorted(src_dir.glob(\"*.jpg\"))\n        linked = 0\n        for jpg in jpgs:\n            if jpg.name not in existing:\n                os.symlink(str(jpg), str(img_dst / jpg.name))\n                linked += 1\n        print(f\"{split}: linked {linked} images (skipped {len(existing)} existing)\")\n\n        # Convert labels\n        json_files = sorted((lbls_root / split).glob(\"*.json\"))\n        print(f\"{split}: converting {len(json_files)} labels...\")\n        work = [(p, lbl_dst) for p in json_files]\n        with ProcessPoolExecutor(max_workers=4) as pool:\n            futures = {pool.submit(convert_one, w): w for w in work}\n            for fut in as_completed(futures):\n                n = fut.result()\n                if n < 0:\n                    total_errors += 1\n                else:\n                    total_objects += n\n\n    print(f\"\\nConverted: {total_objects} objects\")\n    if total_errors > 0:\n        print(f\"Warning: {total_errors} label files failed to parse (skipped)\")\n\n    # Verify\n    n_img = len(list((DATASET / \"images/train\").glob(\"*.jpg\")))\n    n_lbl = len(list((DATASET / \"labels/train\").glob(\"*.txt\")))\n    print(f\"Verification: {n_img} train images, {n_lbl} train labels\")\n    if n_lbl == 0:\n        raise RuntimeError(\"No labels were created. Check your labels dataset structure.\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# DATASET YAML (always recreated so resume works even if /working was cleared)\n# ============================================================\ndataset_yaml = DATASET / \"dataset.yaml\"\n\nif not RESUME_FROM:\n    dataset_yaml.parent.mkdir(parents=True, exist_ok=True)\n    dataset_yaml.write_text(f\"\"\"path: {DATASET}\ntrain: images/train\nval: images/val\n\nnc: 4\nnames:\n  0: vehicle\n  1: pedestrian\n  2: cyclist\n  3: road_obstacle\n\"\"\")\n    print(f\"Created: {dataset_yaml}\")\nelse:\n    print(\"Dataset yaml not needed for resume (config is inside last.pt)\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# TRAIN\n# ============================================================\nfrom ultralytics import YOLO\n\ntrain_ok = False\ntry:\n    if RESUME_FROM:\n        print(f\"Resuming from {RESUME_FROM}\")\n        model = YOLO(RESUME_FROM)\n        model.train(resume=True)\n    else:\n        model = YOLO(\"yolov8n.pt\")\n        model.train(\n            data=str(dataset_yaml),\n            epochs=EPOCHS,\n            imgsz=IMGSZ,\n            batch=BATCH,\n            device=0,\n            project=str(MODELS),\n            name=\"horizon_v1\",\n            exist_ok=True,\n\n            # Safety-critical: maximize recall\n            conf=0.001,\n            iou=0.6,\n\n            # Augmentation for road scenes\n            hsv_h=0.015,\n            hsv_s=0.7,\n            hsv_v=0.4,\n            degrees=0.0,\n            translate=0.1,\n            scale=0.5,\n            fliplr=0.5,\n            flipud=0.0,\n            mosaic=1.0,\n            mixup=0.1,\n\n            # Training\n            optimizer=\"AdamW\",\n            lr0=0.001,\n            lrf=0.01,\n            warmup_epochs=3,\n            weight_decay=0.0005,\n            patience=20,\n            save=True,\n            save_period=5,\n            val=True,\n            plots=True,\n            verbose=True,\n            workers=WORKERS,\n        )\n    train_ok = True\n    print(\"\\nTraining completed successfully.\")\nexcept Exception as e:\n    print(f\"\\nTraining stopped: {e}\")\n    print(\"Attempting to save whatever checkpoint exists...\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# SAVE OUTPUTS (runs even if training was interrupted)\n# ============================================================\nDOWNLOAD.mkdir(exist_ok=True)\n\nweights_dir = MODELS / \"horizon_v1\" / \"weights\"\nif not weights_dir.exists():\n    # Check default ultralytics save location\n    alt = Path(\"/kaggle/working/runs/detect/horizon_v1/weights\")\n    if alt.exists():\n        weights_dir = alt\n\nsaved = []\nif weights_dir.exists():\n    for name in [\"best.pt\", \"last.pt\"]:\n        src = weights_dir / name\n        if src.exists():\n            shutil.copy(src, DOWNLOAD / name)\n            size_mb = src.stat().st_size / 1e6\n            saved.append(f\"{name} ({size_mb:.1f} MB)\")\nelse:\n    print(\"No weights directory found.\")\n\n# Training plots\nresults_dir = weights_dir.parent if weights_dir.exists() else None\nif results_dir and results_dir.exists():\n    for f in results_dir.glob(\"*.png\"):\n        shutil.copy(f, DOWNLOAD / f.name)\n        saved.append(f.name)\n    for f in results_dir.glob(\"*.csv\"):\n        shutil.copy(f, DOWNLOAD / f.name)\n        saved.append(f.name)\n\nprint(f\"Saved to {DOWNLOAD}:\")\nfor s in saved:\n    print(f\"  {s}\")\n\nif not saved:\n    print(\"Nothing saved - training may not have produced any checkpoints.\")\nelif \"last.pt\" in [s.split()[0] for s in saved]:\n    print(\"\\nTo resume: upload last.pt as a Kaggle dataset and set RESUME_FROM in Cell 2.\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# EXPORT TO TFLITE (only if training completed)\n# ============================================================\nbest_path = DOWNLOAD / \"best.pt\"\nif best_path.exists():\n    try:\n        best = YOLO(str(best_path))\n        best.export(format=\"tflite\", imgsz=320, int8=True)\n        # Find the exported file and copy to download\n        for f in Path(\".\").rglob(\"*_saved_model/*.tflite\"):\n            shutil.copy(f, DOWNLOAD / \"horizon_v1_int8.tflite\")\n            print(f\"TFLite exported: {DOWNLOAD / 'horizon_v1_int8.tflite'}\")\n            break\n        else:\n            for f in Path(\".\").rglob(\"*.tflite\"):\n                shutil.copy(f, DOWNLOAD / f.name)\n                print(f\"TFLite exported: {DOWNLOAD / f.name}\")\n                break\n    except Exception as e:\n        print(f\"TFLite export failed: {e}\")\n        print(\"You can export locally: yolo export model=best.pt format=tflite imgsz=320 int8\")\nelse:\n    print(\"No best.pt found, skipping export.\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# QUICK VISUAL TEST (only if best.pt exists)\n# ============================================================\nif best_path.exists():\n    import glob\n    val_dir = DATASET / \"images\" / \"val\"\n    if not val_dir.exists():\n        val_dir = Path(IMAGES_ROOT) / \"val\"\n    test_imgs = sorted(glob.glob(str(val_dir / \"*.jpg\")))[:5]\n    if test_imgs:\n        best = YOLO(str(best_path))\n        results = best.predict(\n            source=test_imgs, conf=0.25, iou=0.6, imgsz=640,\n            save=True, project=str(DOWNLOAD), name=\"test_predictions\", exist_ok=True\n        )\n        for r in results:\n            n = len(r.boxes) if r.boxes is not None else 0\n            print(f\"{Path(r.path).name}: {n} detections\")\n    else:\n        print(\"No val images found for testing.\")\nelse:\n    print(\"No best.pt, skipping test.\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# FINAL SUMMARY\n# ============================================================\nprint(\"=\" * 50)\nprint(\"DOWNLOAD FILES:\")\nif DOWNLOAD.exists():\n    for f in sorted(DOWNLOAD.rglob(\"*\")):\n        if f.is_file():\n            size = f.stat().st_size / 1e6\n            print(f\"  {f.relative_to(DOWNLOAD)}  ({size:.1f} MB)\")\nprint(\"=\" * 50)\nif (DOWNLOAD / \"last.pt\").exists() and not train_ok:\n    print(\"\\nTraining was interrupted!\")\n    print(\"To resume:\")\n    print(\"  1. Download last.pt from this notebook's output\")\n    print(\"  2. Create a new Kaggle dataset from it\")\n    print(\"  3. Set RESUME_FROM in Cell 2 to the path\")\n    print(\"  4. Run all cells again\")\nelif train_ok:\n    print(\"\\nTraining completed! Download best.pt and the .tflite for your Pi5.\")","metadata":{},"outputs":[],"execution_count":null}]}